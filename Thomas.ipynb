{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ultio data analyse\n",
    "\n",
    "*Op dit moment kan ik met Excel of Power BI Ultimo uitlezen. Een sql-query op de database. De functie die ik in Power-BI gebruik is 'ODBC' database. Welke package kan ik gebruiken in Python*\n",
    "\n",
    "Afhankeljk van de database zijn er aparte packages.\n",
    "\n",
    "* https://pypi.org/project/mysql-connector-python/ voor MySQL\n",
    "* sqlite3 voor SQLite (aan te raden voor lightweight projecten)\n",
    "* https://pypi.org/project/pymssql/ voor MSSQL\n",
    "* https://pypi.org/project/cx-Oracle/ voor Oracle\n",
    "* https://pypi.org/project/pyodbc/voor ODBC connecties\n",
    "\n",
    "Maar generieker (en soms iets lastiger) is sqlalchemy. Als je verwacht veel met databases te blijven doen is sqlalchemy een aanrader.\n",
    "\n",
    "*Moeilijkheid zit in tekst velden. \"Ik ben vandaag een patatje wezen halen en heb een FO gerepareed op gemaal huppeldepup\". Locatie, oorzaak en onderdeel moeten tekstmatig herkend worden. Heb wel iets vergelijkbaars gedaan in het verleden met de package \"Fuzzywuzzy\". Alleen is erg traag. Nog suggesties?*\n",
    "\n",
    "* Geen fan van NLTK maar dat wordt wel veel gebruikt voor preprocessing\n",
    "* SKlearn voor basic machine learning algoritmes https://stackabuse.com/text-classification-with-python-and-scikit-learn/\n",
    "* Gebruik een API zoals https://cloud.google.com/natural-language of https://azure.microsoft.com/nl-nl/services/cognitive-services/text-analytics/ \n",
    "\n",
    "De laatste is vaak veel krachtiger dan je zelf kunt bouwen (niet om je machine learning capaciteiten te onderschatten maar omdat microsoft en google en amazon 100en mensen aan de API laten werken!). Het vereist echter wel dat je of een account aangaat (vaak best wel ruim met gratis calls!) en wellicht moet gaan betalen.\n",
    "\n",
    "*Herkennen van groepen en typicals. Welke gemalen hebben veel storingen? (makkelijk)*\n",
    "\n",
    "* Verdiep je goed in numpy \n",
    "* Bij categoriseringsalgoritmes kom je ongetwijfeld datavelden tegen die een categorie voorstellen bv type machine of naam storingsmonteur, je moet deze categorieen altijd omzetten naar integers om machine learning toe te kunnen passen, dit vereist een goede kennis van one hot encoding ( https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html ) Dit kan wel eens lastig zijn in het begin\n",
    "* De meeste data moet genormaliseerd worden om de algoritms goed te krijgen (dus je wilt geen data waarbij bijvoorbeeld kolom 1 van -1 tot +1 loopt en kolom 2 van -1000 tot +5000, hier zijn heel veel algoritmes voor in bv SKlearn\n",
    "* Clusterings algoritmes, begin eens lekker simpel met KMeans (erg eenvoudig algoritme) https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html  \n",
    "* Bouw eerst je kennis uit met KMeans en de aanverwante algoritmen\n",
    "* Bouw een neuraal netwerk voor categorisering (geavanceerd!) bv met Keras of Pytorch\n",
    "\n",
    "*Voorspellen. Ultieme doel.*\n",
    "\n",
    "* Op basis van de algoritmes die je in de vorige stap kunt bouwen is het mogelijk om de data die je gebruikt hebt voor de training van het model in te voeren voor een ander object en daarmee de classificatie uit te voeren. \n",
    "\n",
    "Voorbeeld\n",
    "\n",
    "Je hebt de volgende gegevens van een gemaal;\n",
    "* bouwjaar\n",
    "* laatste onderhoud\n",
    "* trillingen\n",
    "\n",
    "Dan kun je een model bouwen waarmee je clusters maakt van gemalen en hun storingsgevoeligheid. \n",
    "\n",
    "Als je daarna voor een gemaal het laatste onderhoud of de trillingen aanpast kun je opnieuw een voorspelling doen en kan de categorie bv veranderen van 'ok' naar 'storingsgevoelig'.\n",
    "\n",
    "Let wel, onthoud bij elk machine learning model dat je input voor de training gelijk moet zijn aan de input voor de voorspelling. Ik bedoel hiermee dat je in dit fictieve geval dus de gegevens bouwjaar, laatste onderhoud en trillingen moet hebben om een voorspelling te kunnen doen. Je kunt natuurlijk ook weer allerlei onderlinge correlaties bouwen maar hou dit wel in je achterhoofd bij het bouwen! \n",
    "\n",
    "Let ook op dat neurale netwerken heel leuk zijn maar pas eigenlijk goed presteren als er (heel) veel trainingsdata is. Meer een nice to do dan zinvol nu denk ik :-)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
